{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXwwL6hc+Dka1yrzz6WmJD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twjCDWL90u72","executionInfo":{"status":"ok","timestamp":1753787547442,"user_tz":420,"elapsed":953907,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"2b546374-612b-4967-9a94-9afa4c707a0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 6 candidates, totalling 18 fits\n","Test accuracy: 0.9503\n"]}],"source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load MNIST\n","mnist = fetch_openml('mnist_784', version=1)\n","X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n","\n","# Normalize\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train/Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=10000, random_state=42, stratify=y)\n","\n","# Grid Search\n","param_grid = {\n","    'n_neighbors': [3, 4, 5],\n","    'weights': ['uniform', 'distance']\n","}\n","grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, verbose=1, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# Best Model Evaluation\n","best_knn = grid_search.best_estimator_\n","y_pred = best_knn.predict(X_test)\n","print(\"Test accuracy:\", accuracy_score(y_test, y_pred))  # Should be >97%\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","def shift_image(image, direction):\n","    image = image.reshape(28, 28)\n","    shifted = np.zeros_like(image)\n","\n","    if direction == 'left':\n","        shifted[:, :-1] = image[:, 1:]\n","    elif direction == 'right':\n","        shifted[:, 1:] = image[:, :-1]\n","    elif direction == 'up':\n","        shifted[:-1, :] = image[1:, :]\n","    elif direction == 'down':\n","        shifted[1:, :] = image[:-1, :]\n","\n","    return shifted.reshape(-1)\n"],"metadata":{"id":"NyCkmm-01XaX","executionInfo":{"status":"ok","timestamp":1753787560414,"user_tz":420,"elapsed":72,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["X_augmented = []\n","y_augmented = []\n","\n","for img, label in zip(X_train, y_train):\n","    X_augmented.append(img)\n","    y_augmented.append(label)\n","    for direction in ['left', 'right', 'up', 'down']:\n","        shifted = shift_image(img, direction)\n","        X_augmented.append(shifted)\n","        y_augmented.append(label)\n","\n","X_augmented = np.array(X_augmented)\n","y_augmented = np.array(y_augmented)\n","\n","# Train on augmented data\n","X_aug_scaled = scaler.fit_transform(X_augmented)\n","best_knn.fit(X_aug_scaled, y_augmented)\n","y_test_pred = best_knn.predict(X_test)\n","print(\"Test accuracy after augmentation:\", accuracy_score(y_test, y_test_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWPlnCtG1a07","executionInfo":{"status":"ok","timestamp":1753787763034,"user_tz":420,"elapsed":198582,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"3d86c96c-b441-4cc7-d831-82426681f188"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy after augmentation: 0.9618\n"]}]},{"cell_type":"code","source":["import seaborn as sns\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Titanic dataset from seaborn\n","df = sns.load_dataset('titanic')\n","\n","# Drop rows with missing target (Survived)\n","df.dropna(subset=['survived'], inplace=True)\n","\n","# Preprocess\n","df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n","df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n","df['embarked'].fillna(0, inplace=True)\n","df['age'].fillna(df['age'].median(), inplace=True)\n","df['fare'].fillna(df['fare'].median(), inplace=True)\n","\n","# Define features and target\n","features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n","X = df[features]\n","y = df['survived']\n","\n","# Train/Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","\n","# Accuracy\n","print(\"Titanic accuracy:\", accuracy_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mAWQDGh1dzU","executionInfo":{"status":"ok","timestamp":1753787886229,"user_tz":420,"elapsed":1081,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"d7d6dc02-f530-4704-cc9b-82b0f0cabd4f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-5-3209696271.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['embarked'].fillna(0, inplace=True)\n","/tmp/ipython-input-5-3209696271.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['age'].fillna(df['age'].median(), inplace=True)\n","/tmp/ipython-input-5-3209696271.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['fare'].fillna(df['fare'].median(), inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["Titanic accuracy: 0.7623318385650224\n"]}]},{"cell_type":"code","source":["import urllib.request\n","import tarfile\n","import os\n","\n","# URLs for spam and ham data\n","dataset_url = \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\"\n","ham_url = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\"\n","\n","# Create dataset directory\n","os.makedirs(\"datasets\", exist_ok=True)\n","\n","# Download and extract function\n","def download_and_extract(url, extract_path):\n","    archive_path = os.path.join(\"datasets\", os.path.basename(url))\n","    # Download if not already exists\n","    if not os.path.exists(archive_path):\n","        print(f\"Downloading {url}...\")\n","        urllib.request.urlretrieve(url, archive_path)\n","    # Extract\n","    with tarfile.open(archive_path, \"r:bz2\") as tar:\n","        tar.extractall(path=extract_path)\n","    print(f\"Extracted to {extract_path}\")\n","\n","# Download and extract spam and ham\n","download_and_extract(dataset_url, \"datasets\")\n","download_and_extract(ham_url, \"datasets\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6Q7146F1ihX","executionInfo":{"status":"ok","timestamp":1753788013707,"user_tz":420,"elapsed":2576,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"36173c41-0ffa-451c-fc2b-fa65914bc8c4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2...\n","Extracted to datasets\n","Downloading https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2...\n","Extracted to datasets\n"]}]},{"cell_type":"code","source":["from email import policy\n","from email.parser import BytesParser\n","\n","def load_emails(folder):\n","    emails = []\n","    for filename in os.listdir(folder):\n","        filepath = os.path.join(folder, filename)\n","        if os.path.isfile(filepath):  # Avoid subfolders like CVS\n","            with open(filepath, 'rb') as f:\n","                try:\n","                    email = BytesParser(policy=policy.default).parse(f)\n","                    emails.append(email)\n","                except:\n","                    continue  # skip corrupted files\n","    return emails\n","\n","# Load the emails from extracted folders\n","spam_emails = load_emails('datasets/spam')\n","ham_emails = load_emails('datasets/easy_ham')\n","\n","print(f\"Loaded {len(spam_emails)} spam and {len(ham_emails)} ham emails.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNGbmo8t6Xd_","executionInfo":{"status":"ok","timestamp":1753788039604,"user_tz":420,"elapsed":2919,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"6a3d8526-2f69-4a32-c57e-cc40dae1d44e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 501 spam and 2501 ham emails.\n"]}]},{"cell_type":"code","source":["import os\n","import tarfile\n","import urllib.request\n","import re\n","from email import policy\n","from email.parser import BytesParser\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score\n","\n","# --- Step 1: Download and Extract SpamAssassin Dataset ---\n","def download_and_extract(url, extract_to):\n","    filename = os.path.basename(url)\n","    archive_path = os.path.join(\"datasets\", filename)\n","    os.makedirs(\"datasets\", exist_ok=True)\n","    if not os.path.exists(archive_path):\n","        urllib.request.urlretrieve(url, archive_path)\n","    with tarfile.open(archive_path, \"r:bz2\") as tar:\n","        tar.extractall(path=extract_to)\n","\n","download_and_extract(\"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\", \"datasets\")\n","download_and_extract(\"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\", \"datasets\")\n","\n","# --- Step 2: Load Emails ---\n","def load_emails(folder):\n","    emails = []\n","    for filename in os.listdir(folder):\n","        filepath = os.path.join(folder, filename)\n","        if os.path.isfile(filepath):\n","            with open(filepath, 'rb') as f:\n","                try:\n","                    email = BytesParser(policy=policy.default).parse(f)\n","                    emails.append(email)\n","                except:\n","                    continue\n","    return emails\n","\n","spam_emails = load_emails(\"datasets/spam\")\n","ham_emails = load_emails(\"datasets/easy_ham\")\n","\n","# --- Step 3: Email to Text Transformer ---\n","class EmailToTextTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None): return self\n","    def transform(self, X):\n","        return [self.email_to_text(email) for email in X]\n","    def email_to_text(self, email):\n","        try:\n","            return email.get_content()\n","        except:\n","            payload = email.get_payload(decode=True)\n","            if isinstance(payload, bytes):\n","                return payload.decode('utf-8', errors='replace')\n","            return str(payload)\n","\n","# --- Step 4: Text Preprocessing ---\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'\\d+', 'number', text)\n","    text = re.sub(r'http\\S+', 'url', text)\n","    return text\n","\n","# --- Step 5: Combine Labels and Emails ---\n","emails = spam_emails + ham_emails\n","labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n","\n","# --- Step 6: Build and Train Pipeline ---\n","pipeline = Pipeline([\n","    ('email_text', EmailToTextTransformer()),\n","    ('vectorizer', CountVectorizer(preprocessor=preprocess)),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)\n","pipeline.fit(X_train, y_train)\n","\n","# --- Step 7: Evaluate ---\n","y_pred = pipeline.predict(X_test)\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OFiuc071mLe","executionInfo":{"status":"ok","timestamp":1753788198963,"user_tz":420,"elapsed":9198,"user":{"displayName":"Ghuncha Nigar CSE Batch 21","userId":"14768215861040087510"}},"outputId":"f44473b8-9c70-4389-c381-74f30821fdf0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.8294573643410853\n","Recall: 0.8991596638655462\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"i9AvZbTb1pQU"},"execution_count":null,"outputs":[]}]}